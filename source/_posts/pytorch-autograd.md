---
title: PyTorchè‡ªåŠ¨å¾®åˆ†æœºåˆ¶è¯¦è§£
date: 2026-01-25 01:27:00
updated: 2026-01-25 01:27:00
categories:
  - pytorch
tags:
  - PyTorch
  - æ·±åº¦å­¦ä¹ 
  - è‡ªåŠ¨å¾®åˆ†
highlight_shrink: false
---

## ç®€ä»‹

PyTorch çš„è‡ªåŠ¨å¾®åˆ†ï¼ˆAutogradï¼‰æ˜¯å…¶æ ¸å¿ƒåŠŸèƒ½ä¹‹ä¸€ï¼Œå®ƒä½¿å¾—ç¥ç»ç½‘ç»œçš„æ¢¯åº¦è®¡ç®—å˜å¾—ç®€å•é«˜æ•ˆã€‚

<!-- more -->

## å¼ é‡ä¸æ¢¯åº¦

åˆ›å»ºéœ€è¦æ¢¯åº¦çš„å¼ é‡ï¼š

```python
import torch

# ğŸ’¡ requires_grad=True å¯ç”¨æ¢¯åº¦è¿½è¸ª
x = torch.tensor([2.0, 3.0], requires_grad=True)
y = x ** 2 + 2 * x + 1

# è®¡ç®—æ¢¯åº¦
y.backward(torch.ones_like(y))

print(x.grad)  # tensor([6., 8.])
# dy/dx = 2x + 2, å½“ x=[2,3] æ—¶ï¼Œç»“æœä¸º [6, 8]
```

## è®¡ç®—å›¾

PyTorch åŠ¨æ€æ„å»ºè®¡ç®—å›¾ï¼š

```python
import torch

a = torch.tensor([2.0], requires_grad=True)
b = torch.tensor([3.0], requires_grad=True)

c = a * b
d = c + a
e = d ** 2

# åå‘ä¼ æ’­
e.backward()

print(f"a.grad = {a.grad}")  # 2 * (a*b + a) * (b + 1) = 2 * 8 * 4 = 64
print(f"b.grad = {b.grad}")  # 2 * (a*b + a) * a = 2 * 8 * 2 = 32
```

## ç¦ç”¨æ¢¯åº¦è®¡ç®—

æ¨ç†æ—¶ç¦ç”¨æ¢¯åº¦ä»¥èŠ‚çœå†…å­˜ï¼š

```python
# æ–¹æ³•1ï¼šä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨
with torch.no_grad():
    y = model(x)

# æ–¹æ³•2ï¼šä½¿ç”¨è£…é¥°å™¨
@torch.no_grad()
def inference(model, x):
    return model(x)
```

## è‡ªå®šä¹‰å‡½æ•°çš„æ¢¯åº¦

```python
class MyReLU(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input.clamp(min=0)
    
    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input < 0] = 0
        return grad_input
```

## æ€»ç»“

ç†è§£ PyTorch çš„è‡ªåŠ¨å¾®åˆ†æœºåˆ¶æ˜¯æ·±åº¦å­¦ä¹ å¼€å‘çš„åŸºç¡€ã€‚

## å‚è€ƒèµ„æ–™

- [PyTorch Autograd Tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)
