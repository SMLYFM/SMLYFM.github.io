---
title: PINNs å˜ä½“ä¸å‰æ²¿è¿›å±•ï¼šä»åŸºç¡€åˆ° DeepONet
date: 2026-01-25 14:20:00
updated: 2026-01-25 14:20:00
categories:
  - ç§‘å­¦è®¡ç®—
  - åå¾®åˆ†æ–¹ç¨‹
tags:
  - PINNs
  - DeepONet
  - ç¥ç»ç®—å­
  - æ·±åº¦å­¦ä¹ 
  - ç§‘å­¦è®¡ç®—
description: ä»‹ç» PINNs çš„å„ç§å˜ä½“å’Œæ”¹è¿›æ–¹æ³•ï¼ŒåŒ…æ‹¬ hp-VPINNsã€XPINNã€DeepONet ç­‰å‰æ²¿è¿›å±•
mathjax: true
---

## ç®€ä»‹

è‡ª 2019 å¹´ PINNs è®ºæ–‡å‘è¡¨ä»¥æ¥ï¼Œç ”ç©¶è€…ä»¬æå‡ºäº†ä¼—å¤šæ”¹è¿›å˜ä½“æ¥è§£å†³åŸå§‹ PINNs çš„å±€é™æ€§ã€‚æœ¬æ–‡ä»‹ç»å‡ ç§é‡è¦çš„ PINNs å˜ä½“åŠå…¶åº”ç”¨åœºæ™¯ã€‚

<!-- more -->

---

## 1. hp-VPINNsï¼šå˜åˆ†ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ

### 1.1 åŠ¨æœº

åŸå§‹ PINNs ä½¿ç”¨ PDE çš„**å¼ºå½¢å¼**ï¼ˆstrong formï¼‰ï¼Œéœ€è¦è®¡ç®—é«˜é˜¶å¯¼æ•°ã€‚è€Œ **hp-VPINNs** é‡‡ç”¨**å¼±å½¢å¼**ï¼ˆweak formï¼‰ï¼Œé€šè¿‡å˜åˆ†åŸç†å‡å°‘å¯¼æ•°é˜¶æ•°ã€‚

### 1.2 æ•°å­¦åŸç†

å¯¹äºæ¤­åœ†å‹ PDEï¼š

$$
-\nabla \cdot (k \nabla u) = f \quad \text{in } \Omega
$$

å¼ºå½¢å¼éœ€è¦è®¡ç®— $\nabla^2 u$ã€‚è€Œå¼±å½¢å¼ä¸ºï¼š

$$
\int_\Omega k \nabla u \cdot \nabla v \, d\mathbf{x} = \int_\Omega f v \, d\mathbf{x}, \quad \forall v \in V
$$

åªéœ€è¦ä¸€é˜¶å¯¼æ•° $\nabla u$ã€‚

### 1.3 hp ç»†åŒ–ç­–ç•¥

- **h ç»†åŒ–**ï¼šå¢åŠ è®¡ç®—åŸŸçš„å­åŒºåŸŸæ•°é‡
- **p ç»†åŒ–**ï¼šåœ¨æ¯ä¸ªå­åŒºåŸŸå†…å¢åŠ æµ‹è¯•å‡½æ•°çš„å¤šé¡¹å¼æ¬¡æ•°

```python
class VPINNLoss:
    """å˜åˆ† PINN æŸå¤±å‡½æ•°"""
    
    def __init__(self, test_functions):
        self.test_functions = test_functions
    
    def compute(self, model, x, k, f):
        u = model(x)
        u_x = torch.autograd.grad(u, x, 
                                   grad_outputs=torch.ones_like(u),
                                   create_graph=True)[0]
        
        loss = 0
        for v in self.test_functions:
            v_x = torch.autograd.grad(v(x), x,
                                       grad_outputs=torch.ones_like(v(x)),
                                       create_graph=True)[0]
            
            # ğŸ’¡ å¼±å½¢å¼ï¼šâˆ« kâˆ‡uÂ·âˆ‡v dx = âˆ« fv dx
            lhs = torch.mean(k * u_x * v_x)
            rhs = torch.mean(f * v(x))
            loss += (lhs - rhs) ** 2
        
        return loss
```

### 1.4 ä¼˜åŠ¿

- é™ä½å¯¹ç½‘ç»œå…‰æ»‘æ€§çš„è¦æ±‚
- æ›´å®¹æ˜“å¤„ç†å¤æ‚è¾¹ç•Œæ¡ä»¶
- å¯¹éå…‰æ»‘è§£æ›´é²æ£’

---

## 2. XPINNï¼šæ‰©å±•ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ

### 2.1 åŠ¨æœº

åŸå§‹ PINNs éš¾ä»¥å¤„ç†å¤§è§„æ¨¡ã€å¤æ‚å‡ ä½•é—®é¢˜ã€‚**XPINN** é‡‡ç”¨**åŒºåŸŸåˆ†è§£**ç­–ç•¥ï¼Œå°†è®¡ç®—åŸŸåˆ’åˆ†ä¸ºå¤šä¸ªå­åŸŸã€‚

### 2.2 æ–¹æ³•

å°† $\Omega$ åˆ†è§£ä¸º $K$ ä¸ªå­åŸŸï¼š

$$
\Omega = \bigcup_{k=1}^K \Omega_k, \quad \Gamma_{ij} = \partial\Omega_i \cap \partial\Omega_j
$$

æ¯ä¸ªå­åŸŸä½¿ç”¨ç‹¬ç«‹çš„ç¥ç»ç½‘ç»œ $u_k^\theta$ã€‚

### 2.3 ç•Œé¢æ¡ä»¶

åœ¨å­åŸŸç•Œé¢ $\Gamma_{ij}$ ä¸Šæ–½åŠ è¿ç»­æ€§çº¦æŸï¼š

$$
\begin{cases}
u_i(\mathbf{x}) = u_j(\mathbf{x}), & \mathbf{x} \in \Gamma_{ij} \\
\frac{\partial u_i}{\partial n_i}(\mathbf{x}) = -\frac{\partial u_j}{\partial n_j}(\mathbf{x}), & \mathbf{x} \in \Gamma_{ij}
\end{cases}
$$

### 2.4 æŸå¤±å‡½æ•°

$$
\mathcal{L} = \sum_{k=1}^K \mathcal{L}_k^{PDE} + \sum_{i<j} \left( \mathcal{L}_{ij}^{cont} + \mathcal{L}_{ij}^{flux} \right)
$$

```python
class XPINN:
    """æ‰©å±•ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ"""
    
    def __init__(self, n_subdomains):
        self.networks = nn.ModuleList([
            PINN([2, 64, 64, 64, 1]) 
            for _ in range(n_subdomains)
        ])
    
    def interface_loss(self, x_interface, i, j):
        """ç•Œé¢è¿ç»­æ€§æŸå¤±"""
        u_i = self.networks[i](x_interface)
        u_j = self.networks[j](x_interface)
        
        # ğŸ’¡ è§£çš„è¿ç»­æ€§
        loss_cont = torch.mean((u_i - u_j) ** 2)
        
        # ğŸ’¡ é€šé‡çš„è¿ç»­æ€§ï¼ˆéœ€è¦æ³•å‘å¯¼æ•°ï¼‰
        # ... 
        
        return loss_cont
```

### 2.5 ä¼˜åŠ¿

- **å¹¶è¡ŒåŒ–**ï¼šå­åŸŸå¯å¹¶è¡Œè®­ç»ƒ
- **æ‰©å±•æ€§**ï¼šå¯å¤„ç†æ›´å¤§è§„æ¨¡é—®é¢˜
- **çµæ´»æ€§**ï¼šä¸åŒå­åŸŸå¯ä½¿ç”¨ä¸åŒç½‘ç»œæ¶æ„

---

## 3. DeepONetï¼šæ·±åº¦ç®—å­ç½‘ç»œ

### 3.1 åŠ¨æœº

ä¼ ç»Ÿ PINNs é’ˆå¯¹**ç‰¹å®šé—®é¢˜**è®­ç»ƒï¼Œæ— æ³•æ³›åŒ–ã€‚**DeepONet** å­¦ä¹ æ•´ä¸ª**ç®—å­æ˜ å°„**ï¼Œå¯æ³›åŒ–åˆ°ä¸åŒè¾“å…¥å‡½æ•°ã€‚

### 3.2 é€šç”¨é€¼è¿‘å®šç†

Chen & Chen (1995) è¯æ˜ï¼šä»»æ„è¿ç»­ç®—å­ $G$ å¯ä»¥è¡¨ç¤ºä¸ºï¼š

$$
G(u)(y) \approx \sum_{k=1}^p \underbrace{b_k(u(x_1), ..., u(x_m))}_{\text{Branch Net}} \cdot \underbrace{t_k(y)}_{\text{Trunk Net}}
$$

### 3.3 æ¶æ„

DeepONet åŒ…å«ä¸¤ä¸ªå­ç½‘ç»œï¼š

1. **Branch Net**ï¼šç¼–ç è¾“å…¥å‡½æ•° $u$ åœ¨ä¼ æ„Ÿå™¨ç‚¹ $\{x_i\}$ å¤„çš„å€¼
2. **Trunk Net**ï¼šç¼–ç æŸ¥è¯¢ä½ç½® $y$

```python
class DeepONet(nn.Module):
    """æ·±åº¦ç®—å­ç½‘ç»œ"""
    
    def __init__(self, 
                 branch_layers: list[int],
                 trunk_layers: list[int],
                 output_dim: int):
        super().__init__()
        
        # ğŸ’¡ Branch Net: ç¼–ç è¾“å…¥å‡½æ•°
        self.branch_net = self._build_mlp(branch_layers)
        
        # ğŸ’¡ Trunk Net: ç¼–ç æŸ¥è¯¢ä½ç½®
        self.trunk_net = self._build_mlp(trunk_layers)
        
        # ğŸ’¡ è¾“å‡ºç»´åº¦
        self.output_dim = output_dim
    
    def _build_mlp(self, layers):
        modules = []
        for i in range(len(layers) - 1):
            modules.append(nn.Linear(layers[i], layers[i+1]))
            if i < len(layers) - 2:
                modules.append(nn.Tanh())
        return nn.Sequential(*modules)
    
    def forward(self, 
                u_sensors: torch.Tensor,  # [batch, n_sensors]
                y: torch.Tensor           # [batch, dim]
               ) -> torch.Tensor:
        """
        Args:
            u_sensors: è¾“å…¥å‡½æ•°åœ¨ä¼ æ„Ÿå™¨ç‚¹çš„å€¼
            y: æŸ¥è¯¢ä½ç½®
        
        Returns:
            G(u)(y): ç®—å­åœ¨ y å¤„çš„è¾“å‡º
        """
        # ğŸ’¡ Branch è¾“å‡º: [batch, p]
        branch_out = self.branch_net(u_sensors)
        
        # ğŸ’¡ Trunk è¾“å‡º: [batch, p]
        trunk_out = self.trunk_net(y)
        
        # ğŸ’¡ ç‚¹ç§¯åˆæˆ: [batch, 1]
        output = torch.sum(branch_out * trunk_out, dim=-1, keepdim=True)
        return output
```

### 3.4 è®­ç»ƒ

è®­ç»ƒæ•°æ®ä¸ºå¤šç»„ $(u, y, G(u)(y))$ ä¸‰å…ƒç»„ï¼š

```python
def train_deeponet(model, data_loader, epochs=10000):
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    
    for epoch in range(epochs):
        for u_sensors, y, target in data_loader:
            optimizer.zero_grad()
            
            pred = model(u_sensors, y)
            loss = F.mse_loss(pred, target)
            
            loss.backward()
            optimizer.step()
```

### 3.5 Physics-Informed DeepONet

ç»“åˆ PINNs å’Œ DeepONetï¼š

$$
\mathcal{L} = \mathcal{L}_{data} + \lambda_{physics} \mathcal{L}_{physics}
$$

å…¶ä¸­ $\mathcal{L}_{physics}$ æ˜¯ PDE æ®‹å·®æŸå¤±ã€‚

---

## 4. Fourier Neural Operator (FNO)

### 4.1 åŠ¨æœº

DeepONet åœ¨å®è·µä¸­æœ‰æ—¶å—é™äºç‚¹è¯„ä¼°ã€‚**FNO** ç›´æ¥åœ¨å‚…é‡Œå¶ç©ºé—´è¿›è¡Œå·ç§¯ï¼Œæ›´é€‚åˆå‘¨æœŸæ€§é—®é¢˜ã€‚

### 4.2 è°±å·ç§¯

FNO çš„æ ¸å¿ƒæ˜¯**è°±å·ç§¯å±‚**ï¼š

$$
(\mathcal{K}(\phi)v)(x) = \mathcal{F}^{-1}\left( R_\phi \cdot \mathcal{F}(v) \right)(x)
$$

å…¶ä¸­ $\mathcal{F}$ æ˜¯å‚…é‡Œå¶å˜æ¢ï¼Œ$R_\phi$ æ˜¯å¯å­¦ä¹ çš„å‚…é‡Œå¶ç©ºé—´æƒé‡ã€‚

### 4.3 PyTorch å®ç°

```python
class SpectralConv2d(nn.Module):
    """2D è°±å·ç§¯å±‚"""
    
    def __init__(self, in_channels, out_channels, modes1, modes2):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.modes1 = modes1  # ä¿ç•™çš„å‚…é‡Œå¶æ¨¡å¼æ•°
        self.modes2 = modes2
        
        # ğŸ’¡ å‚…é‡Œå¶ç©ºé—´ä¸­çš„å¯å­¦ä¹ æƒé‡
        scale = 1 / (in_channels * out_channels)
        self.weights1 = nn.Parameter(
            scale * torch.rand(in_channels, out_channels, modes1, modes2, 2)
        )
        self.weights2 = nn.Parameter(
            scale * torch.rand(in_channels, out_channels, modes1, modes2, 2)
        )
    
    def compl_mul2d(self, input, weights):
        """å¤æ•°ä¹˜æ³•"""
        return torch.einsum("bixy,ioxy->boxy", input, 
                           torch.view_as_complex(weights))
    
    def forward(self, x):
        batch_size = x.shape[0]
        
        # ğŸ’¡ FFT
        x_ft = torch.fft.rfft2(x)
        
        # ğŸ’¡ åœ¨ä½é¢‘æ¨¡å¼ä¸Šç›¸ä¹˜
        out_ft = torch.zeros(batch_size, self.out_channels, 
                            x.size(-2), x.size(-1)//2 + 1,
                            dtype=torch.cfloat, device=x.device)
        
        out_ft[:, :, :self.modes1, :self.modes2] = \
            self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], 
                            self.weights1)
        out_ft[:, :, -self.modes1:, :self.modes2] = \
            self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], 
                            self.weights2)
        
        # ğŸ’¡ é€† FFT
        x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))
        return x


class FNO2d(nn.Module):
    """2D å‚…é‡Œå¶ç¥ç»ç®—å­"""
    
    def __init__(self, modes1, modes2, width):
        super().__init__()
        self.modes1 = modes1
        self.modes2 = modes2
        self.width = width
        
        # ğŸ’¡ æå‡ç»´åº¦
        self.fc0 = nn.Linear(3, width)  # (x, y, a) -> width
        
        # ğŸ’¡ å‚…é‡Œå¶å±‚
        self.conv0 = SpectralConv2d(width, width, modes1, modes2)
        self.conv1 = SpectralConv2d(width, width, modes1, modes2)
        self.conv2 = SpectralConv2d(width, width, modes1, modes2)
        self.conv3 = SpectralConv2d(width, width, modes1, modes2)
        
        # ğŸ’¡ æ®‹å·®è¿æ¥
        self.w0 = nn.Conv2d(width, width, 1)
        self.w1 = nn.Conv2d(width, width, 1)
        self.w2 = nn.Conv2d(width, width, 1)
        self.w3 = nn.Conv2d(width, width, 1)
        
        # ğŸ’¡ æŠ•å½±è¾“å‡º
        self.fc1 = nn.Linear(width, 128)
        self.fc2 = nn.Linear(128, 1)
    
    def forward(self, x):
        # x: [batch, x, y, 3]
        x = self.fc0(x)
        x = x.permute(0, 3, 1, 2)  # [batch, width, x, y]
        
        # ğŸ’¡ å‚…é‡Œå¶å±‚ + æ®‹å·®
        x1 = self.conv0(x)
        x2 = self.w0(x)
        x = F.gelu(x1 + x2)
        
        x1 = self.conv1(x)
        x2 = self.w1(x)
        x = F.gelu(x1 + x2)
        
        x1 = self.conv2(x)
        x2 = self.w2(x)
        x = F.gelu(x1 + x2)
        
        x1 = self.conv3(x)
        x2 = self.w3(x)
        x = x1 + x2
        
        # ğŸ’¡ æŠ•å½±åˆ°è¾“å‡º
        x = x.permute(0, 2, 3, 1)  # [batch, x, y, width]
        x = self.fc1(x)
        x = F.gelu(x)
        x = self.fc2(x)
        
        return x
```

### 4.4 åº”ç”¨åœºæ™¯

- **Darcy æµ**ï¼šå¤šå­”ä»‹è´¨æµåŠ¨
- **Navier-Stokes**ï¼šæ¹æµé¢„æµ‹
- **æ°”è±¡é¢„æµ‹**ï¼šå…¨çƒå¤©æ°”å»ºæ¨¡

---

## 5. å…¶ä»–å˜ä½“

### 5.1 Conservative PINNs (cPINNs)

é’ˆå¯¹å®ˆæ’å¾‹æ–¹ç¨‹ï¼Œå¼ºåˆ¶æ»¡è¶³å®ˆæ’æ€§è´¨ï¼š

$$
\frac{d}{dt}\int_\Omega u \, dx = -\oint_{\partial\Omega} \mathbf{F} \cdot \mathbf{n} \, dS
$$

### 5.2 Gradient-Enhanced PINNs

åˆ©ç”¨æ¢¯åº¦ä¿¡æ¯å¢å¼ºè®­ç»ƒï¼š

$$
\mathcal{L}_{grad} = \sum_i \left| \frac{\partial u_\theta}{\partial x_i} - \frac{\partial u_{data}}{\partial x_i} \right|^2
$$

### 5.3 Hard-Constrained PINNs

é€šè¿‡ç½‘ç»œæ¶æ„**ç¡¬ç¼–ç **è¾¹ç•Œæ¡ä»¶ï¼š

$$
u_\theta(x) = B(x) + D(x) \cdot N_\theta(x)
$$

å…¶ä¸­ $B(x)$ æ»¡è¶³è¾¹ç•Œæ¡ä»¶ï¼Œ$D(x)$ åœ¨è¾¹ç•Œå¤„ä¸ºé›¶ã€‚

```python
class HardConstrainedPINN(nn.Module):
    """ç¡¬çº¦æŸ PINN"""
    
    def __init__(self, network):
        super().__init__()
        self.network = network
    
    def forward(self, x, t):
        # ğŸ’¡ è·ç¦»å‡½æ•°ï¼šåœ¨è¾¹ç•Œ x=0, x=1 å¤„ä¸º 0
        D = x * (1 - x)
        
        # ğŸ’¡ ç½‘ç»œé¢„æµ‹
        N = self.network(torch.cat([x, t], dim=1))
        
        # ğŸ’¡ è‡ªåŠ¨æ»¡è¶³ Dirichlet è¾¹ç•Œæ¡ä»¶ u(0,t) = u(1,t) = 0
        u = D * N
        return u
```

### 5.4 Transfer Learning for PINNs

åœ¨ç›¸ä¼¼é—®é¢˜é—´è¿ç§»å­¦ä¹ ï¼š

1. åœ¨æºé—®é¢˜ä¸Šé¢„è®­ç»ƒ
2. å†»ç»“å‰å‡ å±‚ï¼Œå¾®è°ƒåå‡ å±‚
3. é€‚åº”ç›®æ ‡é—®é¢˜

---

## 6. å·¥å…·ä¸èµ„æº

### 6.1 å¼€æºæ¡†æ¶

| æ¡†æ¶ | ç‰¹ç‚¹ | é“¾æ¥ |
| ------ | ------ | ------ |
| DeepXDE | å¤šåç«¯æ”¯æŒï¼Œæ˜“ç”¨ | [deepxde.readthedocs.io](https://deepxde.readthedocs.io/) |
| NVIDIA Modulus | å·¥ä¸šçº§ï¼Œå¤šGPU | [developer.nvidia.com/modulus](https://developer.nvidia.com/modulus) |
| SciANN | Keras æ¥å£ | [github.com/sciann/sciann](https://github.com/sciann/sciann) |
| PyDEns | é…ç½®é©±åŠ¨ | [github.com/analysiscenter/pydens](https://github.com/analysiscenter/pydens) |
| NeuralPDE.jl | Julia ç”Ÿæ€ | [github.com/SciML/NeuralPDE.jl](https://github.com/SciML/NeuralPDE.jl) |

### 6.2 Benchmark é—®é¢˜

- **çƒ­ä¼ å¯¼æ–¹ç¨‹**ï¼šéªŒè¯åŸºæœ¬åŠŸèƒ½
- **Burgers æ–¹ç¨‹**ï¼šéçº¿æ€§å¯¹æµ
- **Allen-Cahn æ–¹ç¨‹**ï¼šç›¸åœºæ¨¡å‹
- **Navier-Stokes**ï¼šæµä½“åŠ›å­¦
- **SchrÃ¶dinger æ–¹ç¨‹**ï¼šé‡å­åŠ›å­¦

---

## æ€»ç»“

PINNs å®¶æ—æ­£åœ¨å¿«é€Ÿå‘å±•ï¼š

| å˜ä½“ | æ ¸å¿ƒæ”¹è¿› | é€‚ç”¨åœºæ™¯ |
| ------ | ---------- | ---------- |
| hp-VPINNs | å¼±å½¢å¼ | éå…‰æ»‘è§£ã€å¤æ‚è¾¹ç•Œ |
| XPINN | åŒºåŸŸåˆ†è§£ | å¤§è§„æ¨¡é—®é¢˜ |
| DeepONet | ç®—å­å­¦ä¹  | å‚æ•°åŒ–é—®é¢˜æ— |
| FNO | è°±æ–¹æ³• | å‘¨æœŸæ€§ã€å¤§è§„æ¨¡ |
| cPINNs | å®ˆæ’æ€§ | å®ˆæ’å¾‹æ–¹ç¨‹ |

é€‰æ‹©åˆé€‚çš„å˜ä½“å–å†³äºï¼š

1. **é—®é¢˜è§„æ¨¡**ï¼šå°é—®é¢˜ç”¨ PINNsï¼Œå¤§é—®é¢˜ç”¨ XPINN/FNO
2. **æ•°æ®å¯ç”¨æ€§**ï¼šå°‘æ•°æ®ç”¨ PINNsï¼Œæœ‰æ•°æ®ç”¨ DeepONet
3. **è§£çš„å…‰æ»‘æ€§**ï¼šéå…‰æ»‘ç”¨ hp-VPINNs
4. **æ³›åŒ–éœ€æ±‚**ï¼šéœ€æ³›åŒ–ç”¨ DeepONet/FNO

---

## å‚è€ƒèµ„æ–™

- Kharazmi, E., et al. (2021). hp-VPINNs: Variational physics-informed neural networks with domain decomposition. _CMAME_.
- Jagtap, A. D., & Karniadakis, G. E. (2020). Extended physics-informed neural networks (XPINNs). _CMAME_.
- Lu, L., et al. (2021). Learning nonlinear operators via DeepONet. _Nature Machine Intelligence_.
- Li, Z., et al. (2021). Fourier neural operator for parametric partial differential equations. _ICLR_.
